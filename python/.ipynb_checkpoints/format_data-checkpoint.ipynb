{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format data as JSON**\n",
    "\n",
    "Create 1) nodes with details about each paper and 2) links with source/target representing citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_node(row):\n",
    "    return {\n",
    "        \"id\": row[1].lower() + \" \" + row[0],\n",
    "        \"year\": int(row[0]),\n",
    "        \"title\": row[2],\n",
    "        \"citations\": int(row[3]),\n",
    "        \"social_network\": [r.lower().strip() for r in row[4].split(\",\")],\n",
    "        \"sample_size\": int(row[5]),\n",
    "        \"sample_country\": [r.lower().strip() for r in row[6].split(\",\")],\n",
    "        \"study_type\": [r.lower().strip() for r in row[7].split(\",\")],\n",
    "        \"variables\": [r.lower().strip() for r in row[8].split(\",\")]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = \"../data/data.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'data/data.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-907b2d2be35a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rU'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mrd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'data/data.tsv'"
     ]
    }
   ],
   "source": [
    "in_dataset = ['acquisti & gross 2006', 'banks et al 2010', 'bartsch & dienlin 2016', 'bateman et al 2011', 'benson et al 2015', 'buchi et al 2016', 'collins et al 2012', 'coventry et al 2014', 'cranor et al 1999', 'de wolf et al 2014', 'debatin et al 2009', 'dienlin & trepte 2014', 'dinev & hart 2005', 'doherty & lang 2014', 'dong et al 2015', 'drennan et al 2006', 'dwyer et al 2007', 'feng & xie 2014', 'fox & royne 2018', 'garg et al 2014', 'golbeck & mauriello 2016', 'govani & pashley 2005', 'graeff & harmon 2002', 'hajli & lin 2016', 'hazari & brown 2013', 'heirman et al 2013', 'hoofnagle et al 2010', 'hossain & zhang 2015', 'jensen et al 2005', 'jiang et al 2013', 'johnson et al 2012', 'keith et al 2013', 'kezer et al 2016', 'kisilevich & mansmann 2010', 'krasnova & kift 2012', 'lawler et al 2012', 'lin & liu 2012', 'litt 2013', 'liu et al 2011', 'madejski et al 2012', 'malik et al 2016', 'mcknight et al 2010', 'mesch 2010', 'millham & atkin 2016', 'milne & culnan 2004', 'miltgen & smith 2015', 'moll et al 2014', 'orito et al 2014', 'ozdemir et al 2017', 'paine et al 2007', 'park 2011', 'park et al 2012', 'pitkanen et al 2012', 'posey et al 2010', 'potzch et al 2010', 'reynolds et al 2011', 'staddon et al 2012', 'steijn et al 2016', 'stutzman et al 2011', 'taddicken 2014', 'torres 2012', 'turow 2003', 'tuunainen et al 2009', 'veltri et al 2011', 'wills & zeljkovic 2011', 'xu et al 2008', 'yao et al 2007', 'youn 2009', 'young & quan-haase 2009', 'zlatolas et al 2015']\n",
    "nodes = []\n",
    "links = []\n",
    "\n",
    "with open(filepath, 'rU') as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    for row in rd:\n",
    "        if row[0] != \"year\":\n",
    "            # Make nodes\n",
    "            nodes.append(make_node(row))\n",
    "            citations = [r.lower() for r in row[9].split(\",\")]\n",
    "            for c in citations:\n",
    "                # Make links\n",
    "                link = {\"source\": row[1].lower() + \" \" + row[0], \"target\": c}\n",
    "                if c in in_dataset and link not in links: links.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json.dumps(nodes)\n",
    "json.dumps(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count most common variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variables = []\n",
    "with open(filepath, 'rU') as fd:\n",
    "    rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "    for row in rd:\n",
    "        if row[0] != \"year\":\n",
    "            spl = [r.lower().strip() for r in row[8].split(\",\")]\n",
    "            spl = [s for s in spl if s != \"\"]\n",
    "            variables.extend(spl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "v = Counter(variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "v.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variables = [v.split(\" \") for v in variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "s_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variables = [[v.decode('utf-8', 'ignore') for v in var if v not in s_words] for var in variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variables = [\" \".join([stemmer.stem(v) for v in var]) for var in variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in Counter(variables).most_common(): print x[0] + \"\\t\" + str(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_words = {}\n",
    "for x in variables:\n",
    "    spl = x.split(\" \")\n",
    "    for word in spl:\n",
    "        if word in unique_words:\n",
    "            unique_words[word].append(x)\n",
    "        else:\n",
    "            unique_words[word] = [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in sorted(unique_words):\n",
    "    print len(Counter(unique_words[word]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
